{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.classes import Data, Particles, ParticlesLatent\n",
    "import  pystan\n",
    "import argparse\n",
    "import numpy as np\n",
    "from codebase.file_utils import (\n",
    "    save_obj,\n",
    "    load_obj,\n",
    "    make_folder,\n",
    "    path_backslash\n",
    ")\n",
    "from codebase.ibis import essl, exp_and_normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = Data(\"1factor\", 1, 50)\n",
    "exp_data.generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 1,\n",
       " 'J': 6,\n",
       " 'z': -0.5322712809013522,\n",
       " 'y': array([-1.06227128, -0.0225899 , -1.82581702, -1.66613564, -1.43904415,\n",
       "        -2.64936277]),\n",
       " 'D': array([0, 1, 0, 0, 1, 0])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data.get_stan_data_at_t(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './log/debug/'\n",
    "\n",
    "param_names = ['beta', 'alpha', 'sigma']\n",
    "latent_names = ['z', 'y_latent']\n",
    "particles = ParticlesLatent('normal', 1, 200,  param_names, latent_names, 12, 1)\n",
    "\n",
    "particles.set_log_dir(log_dir)\n",
    "\n",
    "\n",
    "# particles.compile_prior_model()\n",
    "# particles.compile_model()\n",
    "particles.load_prior_model()\n",
    "# particles.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "particles.sample_prior_particles(exp_data.get_stan_data()) # sample prior particles\n",
    "particles.sample_prior_latent_variables(exp_data.get_stan_data())\n",
    "particles.reset_weights()\n",
    "particles.reset_latent_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particles.latent_particles\n",
    "particles.get_latent_weights(exp_data.get_stan_data_at_t(1))\n",
    "# exp_data.get_stan_data_at_t(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 12, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.latent_particles['y_latent'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.resample_particles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.sample_latent_y_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.latent_particles_star['z'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3632687 , 5.31168826, 5.31168826])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.particles['alpha'][:3, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62021104, 2.24447457, 2.24447457])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.latent_particles['z'][:3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup particles\n",
    "particles.set_log_dir(log_dir)\n",
    "# particles.compile_prior_model()\n",
    "# particles.compile_model()\n",
    "particles.load_prior_model()\n",
    "particles.load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e99645b085f461eaef9ae9482f42c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "particles.sample_prior_particles(exp_data.get_stan_data()) # sample prior particles\n",
    "particles.reset_weights() # set weights to 0\n",
    "\n",
    "log_lklhds = np.empty(exp_data.size)\n",
    "degeneracy_limit = 0.5\n",
    "\n",
    "for t in tqdm(range(exp_data.size)):\n",
    "    log_incr_weights = particles.get_incremental_weights(\n",
    "        exp_data.get_stan_data_at_t(t)\n",
    "    )\n",
    "    log_lklhds[t] =  particles.get_loglikelihood_estimate()\n",
    "    particles.update_weights()\n",
    "    \n",
    "    if (essl(particles.weights) < degeneracy_limit * particles.size) and (t+1) < exp_data.size:\n",
    "#         print(\"Deg %d\"%(t))\n",
    "        particles.resample_particles()\n",
    "        particles.jitter(exp_data.get_stan_data_upto_t(t+1))\n",
    "\n",
    "        particles.reset_weights()\n",
    "    else:\n",
    "        particles.update_weights()\n",
    "#     save_obj(particles, 'particles%s'%(t+1), log_dir)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Likelihood 0.00051\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import logsumexp\n",
    "save_obj(log_lklhds, 'log_lklhds', log_dir)\n",
    "marg_lklhd = np.exp(logsumexp(log_lklhds))\n",
    "print('Marginal Likelihood %.5f'%marg_lklhd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Estimate\n",
      "[ 0.86  2.32 -0.25 -2.06 -0.33  4.64]\n",
      "\n",
      "Read Data\n",
      "[ 1.  2. -1. -2.  0.  4.]\n",
      "\n",
      "\n",
      "Estimate\n",
      "[[ 1.13 -0.13  0.08  0.11  0.07 -0.11]\n",
      " [-0.13  4.31  0.11  0.58  0.51  1.32]\n",
      " [ 0.08  0.11  9.87  0.74  0.4  -1.41]\n",
      " [ 0.11  0.58  0.74  1.28 -0.   -0.59]\n",
      " [ 0.07  0.51  0.4  -0.    4.06  1.26]\n",
      " [-0.11  1.32 -1.41 -0.59  1.26  9.58]]\n",
      "\n",
      "Read Data\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 4. 0. 0. 0. 0.]\n",
      " [0. 0. 9. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0. 9.]]\n"
     ]
    }
   ],
   "source": [
    "for name in ['alpha', 'Marg_cov']:\n",
    "    samples = np.squeeze(particles.particles[name])\n",
    "    w = exp_and_normalise(particles.weights)\n",
    "    print('\\n\\nEstimate')\n",
    "    print(np.round(np.average(samples,axis=0, weights=w),2))\n",
    "    print('\\nRead Data')\n",
    "    print(exp_data.raw_data[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Estimate\n",
      "[ 0.84  2.28 -0.23 -2.08 -0.42  4.46]\n",
      "\n",
      "Read Data\n",
      "[ 1.  2. -1. -2.  0.  4.]\n",
      "\n",
      "\n",
      "Estimate\n",
      "[[ 1.17 -0.23  0.13  0.07 -0.08 -0.25]\n",
      " [-0.23  4.25  0.03  0.53  0.44  0.92]\n",
      " [ 0.13  0.03 10.81  0.85  0.35 -1.5 ]\n",
      " [ 0.07  0.53  0.85  1.33 -0.04 -0.64]\n",
      " [-0.08  0.44  0.35 -0.04  3.89  0.58]\n",
      " [-0.25  0.92 -1.5  -0.64  0.58  8.1 ]]\n",
      "\n",
      "Read Data\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 4. 0. 0. 0. 0.]\n",
      " [0. 0. 9. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0. 9.]]\n"
     ]
    }
   ],
   "source": [
    "for name in ['alpha', 'Marg_cov']:\n",
    "    samples = np.squeeze(particles.particles[name])\n",
    "    print('\\n\\nEstimate')\n",
    "    print(np.round(np.mean(samples,axis=0),2))\n",
    "    print('\\nRead Data')\n",
    "    print(exp_data.raw_data[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
