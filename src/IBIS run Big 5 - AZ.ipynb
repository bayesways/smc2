{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.classes import Particles\n",
    "from codebase.classes_data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from codebase.file_utils import (\n",
    "    save_obj,\n",
    "    load_obj,\n",
    "    make_folder,\n",
    "    path_backslash\n",
    ")\n",
    "from codebase.ibis import essl, exp_and_normalise, model_phonebook\n",
    "from tqdm import tqdm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big 5 AZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating new directory: ./log/20201207_153841_big5_az/\n"
     ]
    }
   ],
   "source": [
    "existing_directory = None\n",
    "task_handle = 'big5_az'\n",
    "gen_model = 0\n",
    "\n",
    "if existing_directory is None:\n",
    "    log_dir = make_folder(task_handle)  \n",
    "    print(\"\\n\\nCreating new directory: %s\" % log_dir)\n",
    "\n",
    "else:\n",
    "    log_dir = existing_directory\n",
    "    log_dir = path_backslash(log_dir)\n",
    "    print(\"\\n\\nReading from existing directory: %s\" % log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reading data for women\n",
      "\n",
      "\n",
      "N = 677, J= 15, K =5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate data\n",
    "exp_data = Data(\n",
    "    name = task_handle, \n",
    "    model_num = 'big5', \n",
    "    size = 50,\n",
    "    random_seed = 0\n",
    "    )\n",
    "    \n",
    "exp_data.generate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [5:00:33<00:00, 360.68s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_num = 2\n",
    "## setup particles\n",
    "param_names = model_phonebook(model_num)['param_names']\n",
    "latent_names = model_phonebook(model_num)['latent_names']\n",
    "particles = Particles(\n",
    "    name = 'normal',\n",
    "    model_num = model_num,\n",
    "    size = 1000,\n",
    "    param_names = param_names,\n",
    "    latent_names = latent_names)\n",
    "particles.set_log_dir(log_dir)\n",
    "if gen_model:\n",
    "    particles.compile_prior_model()\n",
    "    particles.compile_model()\n",
    "else:\n",
    "    particles.load_prior_model()\n",
    "    particles.load_model()\n",
    "\n",
    "particles.sample_prior_particles(exp_data.get_stan_data()) # sample prior particles\n",
    "particles.reset_weights() # set weights to 0\n",
    "log_lklhds = np.empty(exp_data.size)\n",
    "degeneracy_limit = 0.5\n",
    "for t in tqdm(range(exp_data.size)):\n",
    "    particles.get_incremental_weights(\n",
    "        exp_data.get_stan_data_at_t(t)\n",
    "        )\n",
    "    log_lklhds[t] =  particles.get_loglikelihood_estimate()\n",
    "    particles.update_weights()\n",
    "    \n",
    "    if (essl(particles.weights) < degeneracy_limit * particles.size) and (t+1) < exp_data.size:\n",
    "        particles.resample_particles()\n",
    "        particles.jitter(exp_data.get_stan_data_upto_t(t+1))\n",
    "        particles.reset_weights()\n",
    "    else:\n",
    "        particles.update_weights()\n",
    "\n",
    "save_obj(log_lklhds, 'log_lklhds', log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Marginal Likelihood 0.00000\n",
      "\n",
      "\n",
      "Estimate\n",
      "[ 0.03 -0.17  0.08 -0.03  0.19  0.23 -0.02 -0.   -0.06  0.06 -0.22 -0.2\n",
      "  0.19  0.07  0.15]\n",
      "\n",
      "\n",
      "Estimate\n",
      "[[ 0.57  0.19  0.13  0.12  0.07  0.06  0.07  0.03 -0.02  0.    0.07 -0.08\n",
      "   0.11  0.13  0.09]\n",
      " [ 0.19  1.17  0.35  0.26  0.12  0.27  0.29  0.15  0.01  0.04  0.1  -0.29\n",
      "   0.3   0.29  0.3 ]\n",
      " [ 0.13  0.35  0.77  0.22  0.1   0.39  0.21  0.11 -0.1   0.04  0.01 -0.26\n",
      "   0.17  0.1   0.19]\n",
      " [ 0.12  0.26  0.22  1.04  0.1   0.26  0.16  0.08 -0.02  0.04  0.04 -0.22\n",
      "   0.3   0.22  0.24]\n",
      " [ 0.07  0.12  0.1   0.1   0.99  0.09  0.09  0.07 -0.    0.01  0.03 -0.09\n",
      "   0.1   0.08  0.09]\n",
      " [ 0.06  0.27  0.39  0.26  0.09  0.86  0.18  0.09 -0.1   0.05 -0.08 -0.27\n",
      "   0.19  0.06  0.19]\n",
      " [ 0.07  0.29  0.21  0.16  0.09  0.18  0.87  0.4   0.2   0.02 -0.01 -0.25\n",
      "   0.17  0.12  0.23]\n",
      " [ 0.03  0.15  0.11  0.08  0.07  0.09  0.4   0.97  0.35 -0.03 -0.06 -0.21\n",
      "   0.06 -0.01  0.15]\n",
      " [-0.02  0.01 -0.1  -0.02 -0.   -0.1   0.2   0.35  1.18 -0.05 -0.05 -0.05\n",
      "   0.04  0.    0.08]\n",
      " [ 0.    0.04  0.04  0.04  0.01  0.05  0.02 -0.03 -0.05  1.02  0.15  0.1\n",
      "   0.08  0.09  0.07]\n",
      " [ 0.07  0.1   0.01  0.04  0.03 -0.08 -0.01 -0.06 -0.05  0.15  0.94  0.24\n",
      "   0.1   0.18  0.02]\n",
      " [-0.08 -0.29 -0.26 -0.22 -0.09 -0.27 -0.25 -0.21 -0.05  0.1   0.24  1.13\n",
      "  -0.2  -0.11 -0.25]\n",
      " [ 0.11  0.3   0.17  0.3   0.1   0.19  0.17  0.06  0.04  0.08  0.1  -0.2\n",
      "   1.11  0.53  0.45]\n",
      " [ 0.13  0.29  0.1   0.22  0.08  0.06  0.12 -0.01  0.    0.09  0.18 -0.11\n",
      "   0.53  1.17  0.45]\n",
      " [ 0.09  0.3   0.19  0.24  0.09  0.19  0.23  0.15  0.08  0.07  0.02 -0.25\n",
      "   0.45  0.45  0.95]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n\\n')\n",
    "marg_lklhd = np.exp(logsumexp(log_lklhds))\n",
    "print('Marginal Likelihood %.5f'%marg_lklhd)\n",
    "\n",
    "for name in ['alpha', 'Marg_cov']:\n",
    "    samples = np.squeeze(particles.particles[name])\n",
    "    w = exp_and_normalise(particles.weights)\n",
    "    print('\\n\\nEstimate')\n",
    "    print(np.round(np.average(samples,axis=0, weights=w),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.634929751556244e-06"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg_lklhd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
