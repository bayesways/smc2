{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.classes import Data, Particles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from codebase.file_utils import (\n",
    "    save_obj,\n",
    "    load_obj,\n",
    "    make_folder,\n",
    "    path_backslash\n",
    ")\n",
    "from codebase.ibis import essl, exp_and_normalise, model_phonebook\n",
    "from tqdm import tqdm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big 5 EZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating new directory: ./log/20201205_201808_big5/\n"
     ]
    }
   ],
   "source": [
    "existing_directory = None\n",
    "task_handle = 'big5_ez'\n",
    "gen_model = 0\n",
    "\n",
    "if existing_directory is None:\n",
    "    log_dir = make_folder(task_handle)  \n",
    "    print(\"\\n\\nCreating new directory: %s\" % log_dir)\n",
    "\n",
    "else:\n",
    "    log_dir = existing_directory\n",
    "    log_dir = path_backslash(log_dir)\n",
    "    print(\"\\n\\nReading from existing directory: %s\" % log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reading data for women\n",
      "\n",
      "\n",
      "N = 677, J= 15, K =5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate data\n",
    "exp_data = Data(\n",
    "    name = task_handle, \n",
    "    model_num = 'big5', \n",
    "    size = 50,\n",
    "    random_seed = 0\n",
    "    )\n",
    "    \n",
    "exp_data.generate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:50:26<00:00, 132.53s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_num = 1\n",
    "## setup particles\n",
    "param_names = model_phonebook(model_num)['param_names']\n",
    "latent_names = model_phonebook(model_num)['latent_names']\n",
    "particles = Particles(\n",
    "    name = 'big5',\n",
    "    model_num = model_num,\n",
    "    size = 1000,\n",
    "    param_names = param_names,\n",
    "    latent_names = latent_names)\n",
    "particles.set_log_dir(log_dir)\n",
    "if gen_model:\n",
    "    particles.compile_prior_model()\n",
    "    particles.compile_model()\n",
    "else:\n",
    "    particles.load_prior_model()\n",
    "    particles.load_model()\n",
    "\n",
    "particles.sample_prior_particles(exp_data.get_stan_data()) # sample prior particles\n",
    "particles.reset_weights() # set weights to 0\n",
    "log_lklhds = np.empty(exp_data.size)\n",
    "degeneracy_limit = 0.5\n",
    "for t in tqdm(range(exp_data.size)):\n",
    "    particles.get_incremental_weights(\n",
    "        exp_data.get_stan_data_at_t(t)\n",
    "        )\n",
    "    log_lklhds[t] =  particles.get_loglikelihood_estimate()\n",
    "    particles.update_weights()\n",
    "    \n",
    "    if (essl(particles.weights) < degeneracy_limit * particles.size) and (t+1) < exp_data.size:\n",
    "        particles.resample_particles()\n",
    "        particles.jitter(exp_data.get_stan_data_upto_t(t+1))\n",
    "        particles.reset_weights()\n",
    "    else:\n",
    "        particles.update_weights()\n",
    "\n",
    "save_obj(log_lklhds, 'log_lklhds', log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Marginal Likelihood 0.00000\n",
      "\n",
      "\n",
      "Estimate\n",
      "[ 0.02 -0.18  0.09  0.01  0.21  0.16 -0.08 -0.05 -0.04 -0.04 -0.2  -0.11\n",
      "  0.13  0.1   0.09]\n",
      "\n",
      "\n",
      "Estimate\n",
      "[[ 0.58  0.2   0.19  0.07  0.05  0.11  0.05  0.09  0.04 -0.04 -0.04 -0.1\n",
      "   0.06  0.09  0.08]\n",
      " [ 0.2   1.12  0.33  0.13  0.1   0.19  0.09  0.17  0.08 -0.08 -0.07 -0.18\n",
      "   0.12  0.16  0.15]\n",
      " [ 0.19  0.33  0.88  0.12  0.09  0.19  0.09  0.16  0.08 -0.07 -0.07 -0.16\n",
      "   0.1   0.14  0.13]\n",
      " [ 0.07  0.13  0.12  0.93  0.1   0.23  0.05  0.08  0.04 -0.05 -0.05 -0.12\n",
      "   0.09  0.12  0.11]\n",
      " [ 0.05  0.1   0.09  0.1   0.87  0.15  0.03  0.06  0.03 -0.04 -0.04 -0.09\n",
      "   0.07  0.09  0.08]\n",
      " [ 0.11  0.19  0.19  0.23  0.15  0.98  0.07  0.12  0.06 -0.08 -0.08 -0.2\n",
      "   0.14  0.18  0.17]\n",
      " [ 0.05  0.09  0.09  0.05  0.03  0.07  0.73  0.32  0.15 -0.06 -0.06 -0.14\n",
      "   0.09  0.11  0.11]\n",
      " [ 0.09  0.17  0.16  0.08  0.06  0.12  0.32  0.92  0.27 -0.11 -0.12 -0.26\n",
      "   0.15  0.19  0.19]\n",
      " [ 0.04  0.08  0.08  0.04  0.03  0.06  0.15  0.27  1.13 -0.05 -0.05 -0.11\n",
      "   0.07  0.09  0.09]\n",
      " [-0.04 -0.08 -0.07 -0.05 -0.04 -0.08 -0.06 -0.11 -0.05  1.05  0.11  0.24\n",
      "  -0.05 -0.07 -0.07]\n",
      " [-0.04 -0.07 -0.07 -0.05 -0.04 -0.08 -0.06 -0.12 -0.05  0.11  0.94  0.25\n",
      "  -0.05 -0.07 -0.07]\n",
      " [-0.1  -0.18 -0.16 -0.12 -0.09 -0.2  -0.14 -0.26 -0.11  0.24  0.25  1.06\n",
      "  -0.13 -0.17 -0.17]\n",
      " [ 0.06  0.12  0.1   0.09  0.07  0.14  0.09  0.15  0.07 -0.05 -0.05 -0.13\n",
      "   0.93  0.35  0.33]\n",
      " [ 0.09  0.16  0.14  0.12  0.09  0.18  0.11  0.19  0.09 -0.07 -0.07 -0.17\n",
      "   0.35  1.11  0.43]\n",
      " [ 0.08  0.15  0.13  0.11  0.08  0.17  0.11  0.19  0.09 -0.07 -0.07 -0.17\n",
      "   0.33  0.43  0.82]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n\\n')\n",
    "marg_lklhd = np.exp(logsumexp(log_lklhds))\n",
    "print('Marginal Likelihood %.5f'%marg_lklhd)\n",
    "\n",
    "for name in ['alpha', 'Marg_cov']:\n",
    "    samples = np.squeeze(particles.particles[name])\n",
    "    w = exp_and_normalise(particles.weights)\n",
    "    print('\\n\\nEstimate')\n",
    "    print(np.round(np.average(samples,axis=0, weights=w),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3916996226010373e-06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg_lklhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
